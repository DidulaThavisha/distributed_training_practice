[data]
data_path = "./data"

[job]
dump_folder = "./outputs"
description = "LLM debug model"
print_args = false
use_for_integration_test = true

[profiling]
enable_profiling = false
save_traces_folder = "profile_trace"
profile_freq = 10
enable_memory_snapshot = false
save_memory_snapshot_folder = "memory_snapshot"

[metrics]
log_freq = 1
disable_color_printing = false
enable_tensorboard = true
save_tb_folder = "tb"
enable_wandb = false

[model]
name = "llm"
flavor = "llm_small"

[optimizer]
name = "AdamW"
lr = 8e-4
eps = 1e-8

[lr_scheduler]
warmup_steps = 1  # 10% warmup steps
decay_ratio = 0.0  # no decay, stay stable during training

[training]
batch_size = 8
seq_len = 512
max_norm = 2.0  # grad norm clipping
steps = 100
compile = true
dataset = "./data/train/pile_train.h5"
classifer_free_guidance_prob = 0.1



[eval]
enable_classifer_free_guidance = true
classifer_free_guidance_scale = 5.0
denoising_steps = 4
save_img_folder = "img"
eval_freq = 5

[parallelism]
data_parallel_replicate_degree = 1
data_parallel_shard_degree = 2


[experimental]
custom_args_module = "llm_argparser"

[activation_checkpoint]
mode = "full"
